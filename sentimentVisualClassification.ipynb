{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of visualClassification_NEW!!!!!!",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRhHzBvLjXV4",
        "colab_type": "text"
      },
      "source": [
        "# Visual Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt8EHYT2LiiM",
        "colab_type": "text"
      },
      "source": [
        "## **Test use GPU**\n",
        "\n",
        "Go to Runtime\n",
        "\n",
        "select accelerator: GPU\n",
        "\n",
        "SAVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnW0xi9nLjOV",
        "colab_type": "code",
        "outputId": "71c01df4-2a3d-4ce6-a986-44fceb959942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHfnoOlG6eUd",
        "colab_type": "text"
      },
      "source": [
        "## **Transfer files from gdrive to colaborator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyTqrIU3jb6o",
        "colab_type": "text"
      },
      "source": [
        "###**Google Drive Authenticate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2So3kksyJes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#1.Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9da3afGMj-Hd",
        "colab_type": "text"
      },
      "source": [
        "###**Load Session**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzA1bAEikA9X",
        "colab_type": "text"
      },
      "source": [
        "create directory tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1duGvIgAj95_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mkdir models\n",
        "!mkdir results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR0eajm1kI5_",
        "colab_type": "text"
      },
      "source": [
        "Loading dataset file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQq7rGEGfr00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. Load files by ID from Google Drive:\n",
        "#  occorre caricare i file su Google Drive, poi renderli pubblici e copiare l'id del file dal loro link pubblico\n",
        "\n",
        "#Esempio:\n",
        "# Dataset.zip <=> https://drive.google.com/open?id=1kJwMvgml5haYrTpr_30n0enUMPX-UyT9\n",
        "\n",
        "idfile=\"1kJwMvgml5haYrTpr_30n0enUMPX-UyT9\" #\n",
        "downloaded = drive.CreateFile({'id': idfile})\n",
        "downloaded.GetContentFile('Dataset.zip') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29KeC6WPkeo0",
        "colab_type": "text"
      },
      "source": [
        "Extract tree and files of the session "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grJRO6n2Sdeu",
        "colab_type": "code",
        "outputId": "b6dbf9c9-c058-4ca9-ec8e-ae7c4d41d84a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150330
        }
      },
      "source": [
        "#Decompressione dei file caricati\n",
        "!unzip Dataset.zip\n",
        "#!rm Dataset.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Dataset.zip\n",
            "   creating: Dataset/\n",
            "  inflating: dataset.txt             \n",
            "  inflating: Dataset/img00001.jpg    \n",
            "  inflating: Dataset/img00002.jpg    \n",
            "  inflating: Dataset/img00003.jpg    \n",
            "  inflating: Dataset/img00004.jpg    \n",
            "  inflating: Dataset/img00005.jpg    \n",
            "  inflating: Dataset/img00006.jpg    \n",
            "  inflating: Dataset/img00007.jpg    \n",
            "  inflating: Dataset/img00008.jpg    \n",
            "  inflating: Dataset/img00009.jpg    \n",
            "  inflating: Dataset/img00010.jpg    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-R5uNtnvMZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Controllo se i file sono stati caricati e decompressi correttamente:\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZWCbge3m5ir",
        "colab_type": "text"
      },
      "source": [
        "### Resizing and Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOpW1WXnm3OG",
        "colab_type": "code",
        "outputId": "46fc7358-4f6f-443d-8349-ab38d6f409d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def scale(image, max_size, method=Image.ANTIALIAS):\n",
        "    \"\"\"\n",
        "    resize 'image' to 'max_size' keeping the aspect ratio \n",
        "    and place it in center of white 'max_size' image \n",
        "    \"\"\"\n",
        "    im_aspect = float(image.size[0])/float(image.size[1])\n",
        "    out_aspect = float(max_size[0])/float(max_size[1])\n",
        "    if im_aspect >= out_aspect:\n",
        "        scaled = image.resize((max_size[0], int((float(max_size[0])/im_aspect) + 0.5)), method)\n",
        "    else:\n",
        "        scaled = image.resize((int((float(max_size[1])*im_aspect) + 0.5), max_size[1]), method)\n",
        " \n",
        "    offset = (((max_size[0] - scaled.size[0]) / 2), ((max_size[1] - scaled.size[1]) / 2))\n",
        "    back = Image.new(\"RGB\", max_size, \"black\")\n",
        "    back.paste(scaled, offset)\n",
        "    return back\n",
        "\n",
        "\n",
        "\n",
        "path_results=\"resized\"\n",
        "if not os.path.exists(path_results):\n",
        "    os.mkdir(path_results)\n",
        "\n",
        "files=glob.glob(\"Dataset/*\")\n",
        "for i,f in enumerate(files):\n",
        "    \n",
        "    if i%500==0: print(\"{} of {}\".format(i,len(files)))\n",
        "    try:\n",
        "        im = Image.open(f)\n",
        "        max_size=max(im.size)\n",
        "        #print(\"from {} to {}\".format(im.size,(max_size,max_size)))\n",
        "        scaled=scale(im,(max_size,max_size))\n",
        "        resized=scaled.resize((224,224))\n",
        "        resized.save(path_results+\"/\"+os.path.basename(f))\n",
        "        #raw_input()\n",
        "    except:\n",
        "        print(\"Error: {}\".format(f))\n",
        "print(\"Done!!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 9248\n",
            "500 of 9248\n",
            "1000 of 9248\n",
            "1500 of 9248\n",
            "2000 of 9248\n",
            "2500 of 9248\n",
            "3000 of 9248\n",
            "3500 of 9248\n",
            "4000 of 9248\n",
            "4500 of 9248\n",
            "5000 of 9248\n",
            "5500 of 9248\n",
            "6000 of 9248\n",
            "6500 of 9248\n",
            "7000 of 9248\n",
            "7500 of 9248\n",
            "8000 of 9248\n",
            "Error: Dataset/img03312.jpg\n",
            "8500 of 9248\n",
            "9000 of 9248\n",
            "Done!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImcOx484Iojy",
        "colab_type": "code",
        "outputId": "57bde167-df6c-485e-97ae-79a688fbcfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!rm -r Dataset\n",
        "!mv resized Dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'resized': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "larRIOIzjLgY",
        "colab_type": "text"
      },
      "source": [
        "## Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEMQpq7wjuB3",
        "colab_type": "text"
      },
      "source": [
        "###**Loading common modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9C9udfDgd7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c1c63d2e-1c52-4ad0-e856-fb9d476a7dc9"
      },
      "source": [
        "import keras\n",
        "#from keras.applications.vgg16 import VGG16\n",
        "#from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "from keras.legacy import interfaces\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "save_path=\"results/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "    \n",
        "basepath=save_path+\"finetuning\"\n",
        "modello=basepath+\"-e{epoch:02d}-vba{val_acc:.2f}.h5\"\n",
        "csv=basepath+\"-finetuning.csv\"\n",
        "plot1=basepath+\"-plot1.png\"\n",
        "\n",
        "classeMax=3  #Quante classi devo classificare\n",
        "\n",
        "\n",
        "encoded_labels = to_categorical(range(classeMax)) #Codifico le label in one-hot vectors"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d113OJr_j10H",
        "colab_type": "text"
      },
      "source": [
        "### Data managemennt and manipolation (preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIwKyTZekq5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadDataset(datasetFile):\n",
        "    \"\"\"\n",
        "    ritorna:\n",
        "    - la lista dei nomi dei file immagine\n",
        "    - visual sentiment\n",
        "    -..\n",
        "    - la lista dei testi in chiaro contenuti nell'immagini\n",
        "    \n",
        "    datasetFile: percorso del file\n",
        "    \"\"\"\n",
        "            \n",
        "    datasetList= pd.read_csv(datasetFile, sep=' ', quotechar=\"@\", header=None, names = ['imgPath', 'V', 'Tx', 'O', 'text'])\n",
        "        \n",
        "    return datasetList\n",
        "\n",
        "\n",
        "def shuffling(datasetList, shuffle=1):\n",
        "    if shuffle==1:\n",
        "        datasetList= datasetList.sample(frac=1) \n",
        "        \n",
        "    return datasetList   \n",
        "    \n",
        "        \n",
        "def splitTrainValidationSet(datasetList, r, shuffle=1):\n",
        "    \"\"\"\n",
        "    ritorna la lista dei file da destinare al training set e quelli da destinare al \n",
        "    validation set\n",
        "    \n",
        "    classifiedFileList: lista dei file\n",
        "    r: rapporto di slitting (training/validation samples)\n",
        "    shuffle: (default 1), casualizza l'ordine dei file di ogni classe prima dello split\n",
        "    \"\"\"\n",
        "    \n",
        "    datasetListShuffled= shuffling(datasetList, shuffle)\n",
        "\n",
        "    idx = int(len(datasetListShuffled)*(1-r)) # dimensione del set di validazione per classe\n",
        "    trainClassifiedFileList= datasetListShuffled[idx:]\n",
        "    valClassifiedFileList= datasetListShuffled[:idx]\n",
        "        \n",
        "    return trainClassifiedFileList, valClassifiedFileList\n",
        "    \n",
        "    \n",
        "def toBalance(datasetList, classificationColumn, shuffle=1):\n",
        "    \"\"\"\n",
        "    ritorna la lista dei file bilanciata (stesso numero di file per ciascuna classe).\n",
        "    Il numero dei file per ogni classe sara' uguale al numero di file della classe \n",
        "    col minor numero di elementi\n",
        "    \n",
        "    fileList: lista dei file\n",
        "    shuffle: (default 1), casualizza l'ordine dei file di ogni classe\n",
        "    \"\"\"\n",
        "    datasetListShuffle= shuffling(datasetList, shuffle)\n",
        "    \n",
        "    classLabels= np.unique(datasetListShuffle[classificationColumn].values)\n",
        "    \n",
        "    classFileLen=[]\n",
        "    classIndexes=[]\n",
        "    for i in classLabels:\n",
        "        indexes= datasetListShuffle.index.values[datasetListShuffle[classificationColumn].values==i]\n",
        "        classLen= len(indexes)\n",
        "        classIndexes.append(indexes)\n",
        "        classFileLen.append(classLen)\n",
        "        \n",
        "    minLen= min(classFileLen)\n",
        "    \n",
        "    indexes= np.ndarray((0,), 'int')\n",
        "    for i in range(len(classFileLen)):\n",
        "        ind= classIndexes[i][:minLen]\n",
        "        indexes= np.append(indexes,ind)\n",
        "        \n",
        "    balancedDatasetList= datasetListShuffle.loc[indexes]\n",
        "    \n",
        "    return balancedDatasetList\n",
        "  \n",
        "\n",
        "def loadInputImages(fileNames, dirPath=\"\"):\n",
        "    \"\"\"\n",
        "    carica in memoria il file (nel formato opportuno) al percorso \"fileName\"\n",
        "    \"\"\"\n",
        "    #images=np.ndarray((0, 224, 224, 3))\n",
        "    images=[]\n",
        "    for fileName in fileNames:\n",
        "        #image= scipy.misc.imread(join(dirPath,fileName))\n",
        "        img = image.load_img(join(dirPath,fileName), target_size=(224, 224),grayscale=False)\n",
        "        img = image.img_to_array(img)\n",
        "        #images= np.images(images, [img], axis=0)\n",
        "        images.append(img)\n",
        "     \n",
        "    return np.array(images)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei01Jy8zlLyP",
        "colab_type": "text"
      },
      "source": [
        "### Ausiliar Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX37KKWZp_sM",
        "colab_type": "text"
      },
      "source": [
        "Definizione \"nuovo\" algoritmo SGD (permette Multiple Learning Rates) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VTwPLeyqAPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR_SGD(Optimizer):\n",
        "    \"\"\"Stochastic gradient descent optimizer.\n",
        "\n",
        "    Includes support for momentum,\n",
        "    learning rate decay, and Nesterov momentum.\n",
        "\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        momentum: float >= 0. Parameter updates momentum.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0., decay=0.,\n",
        "                 nesterov=False,multipliers=None,**kwargs):\n",
        "        super(LR_SGD, self).__init__(**kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.lr = K.variable(lr, name='lr')\n",
        "            self.momentum = K.variable(momentum, name='momentum')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "        self.initial_decay = decay\n",
        "        self.nesterov = nesterov\n",
        "        self.lr_multipliers = multipliers\n",
        "\n",
        "    @interfaces.legacy_get_updates_support\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                  K.dtype(self.decay))))\n",
        "        # momentum\n",
        "        shapes = [K.int_shape(p) for p in params]\n",
        "        moments = [K.zeros(shape) for shape in shapes]\n",
        "        self.weights = [self.iterations] + moments\n",
        "        for p, g, m in zip(params, grads, moments):\n",
        "            \n",
        "            matched_layer = [x for x in self.lr_multipliers.keys() if x in p.name]\n",
        "            if matched_layer:\n",
        "                new_lr = lr * self.lr_multipliers[matched_layer[0]]\n",
        "            else:\n",
        "                new_lr = lr\n",
        "\n",
        "            v = self.momentum * m - new_lr * g  # velocity\n",
        "            self.updates.append(K.update(m, v))\n",
        "\n",
        "            if self.nesterov:\n",
        "                new_p = p + self.momentum * v - new_lr * g\n",
        "            else:\n",
        "                new_p = p + v\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'momentum': float(K.get_value(self.momentum)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'nesterov': self.nesterov}\n",
        "        base_config = super(LR_SGD, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxm8ExDXlcYP",
        "colab_type": "text"
      },
      "source": [
        "### Settaggi iniziali"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFOegypNlecr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "2458a34f-ba0a-4bd5-aa87-399a3a822e94"
      },
      "source": [
        "# DATA E PREPROCESSING\n",
        "dataPath=''\n",
        "\n",
        "# MODEL\n",
        "# create the base pre-trained model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "base_model = VGG16(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "#from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "#base_model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "#from keras.applications.densenet import DenseNet121\n",
        "#base_model = DenseNet121(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "#from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "#base_model = InceptionResNetV2(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "#from keras.applications.xception import Xception\n",
        "#base_model = Xception(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "#from keras.applications.mobilenet import MobileNet\n",
        "#base_model = MobileNet(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
        "x = base_model.output\n",
        "# Classification block\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "predictions = Dense(classeMax, activation='softmax', name='predictions')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# loading weights from pretrained model \n",
        "#model.load_weights(\"results/epoc72.hdf5\") #####\n",
        "\n",
        "\n",
        "# Setting trainability of layers\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = True\n",
        "  #print(layer.trainable)\n",
        "  #print(layer.__class__.__name__)\n",
        "\n",
        "  \n",
        "# Setting the Learning rate multipliers\n",
        "\n",
        "LR_mult_dict = {}\n",
        "LR_mult_dict['block1_conv1']=0.1\n",
        "LR_mult_dict['block1_conv2']=0.1\n",
        "LR_mult_dict['block2_conv1']=0.2\n",
        "LR_mult_dict['block2_conv2']=0.2\n",
        "LR_mult_dict['block3_conv1']=0.3\n",
        "LR_mult_dict['block3_conv2']=0.3\n",
        "LR_mult_dict['block3_conv3']=0.3\n",
        "LR_mult_dict['block4_conv1']=0.5\n",
        "LR_mult_dict['block4_conv2']=0.5\n",
        "LR_mult_dict['block4_conv3']=0.5\n",
        "LR_mult_dict['block5_conv1']=0.7\n",
        "LR_mult_dict['block5_conv2']=0.7\n",
        "LR_mult_dict['block5_conv3']=0.7\n",
        "LR_mult_dict['fc1']=1\n",
        "LR_mult_dict['fc2']=1\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "batch_size = 32\n",
        "lossFunction='categorical_crossentropy'\n",
        "base_lr = 1e-3 #0.00001\n",
        "momentum = 0.9\n",
        "#optimizer = SGD(lr=base_lr, decay=0.0, momentum=momentum, nesterov=True)\n",
        "optimizer = LR_SGD(lr=base_lr, momentum=momentum, decay=0.0, nesterov=False,multipliers = LR_mult_dict)\n",
        "epochs= 250 #2000 #250\n",
        "\n",
        "\n",
        "# TEST\n",
        "val_batch_size = 32\n",
        "metrics=[\"accuracy\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSodk3lpek9R",
        "colab_type": "text"
      },
      "source": [
        "###**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPq65Q75lgGU",
        "colab_type": "code",
        "outputId": "0d1d346d-1a34-4ede-ceb3-2ae98f6cf2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "# LOADING TRAINING DATA E PREPROCESSING\n",
        "datasetFile=\"dataset.txt\"\n",
        "datasetList= loadDataset(datasetFile)\n",
        "\n",
        "balancedDatasetList= toBalance(datasetList, \"V\", shuffle=1)\n",
        "\n",
        "r= 0.8\n",
        "trainSetList, valSetList= splitTrainValidationSet(balancedDatasetList, r, shuffle=1)\n",
        "\n",
        "print(len(trainSetList))\n",
        "print(len(trainSetList[trainSetList.V==0]))\n",
        "print(len(trainSetList[trainSetList.V==1]))\n",
        "print(len(trainSetList[trainSetList.V==2]))\n",
        "\n",
        "print(len(valSetList))\n",
        "print(len(valSetList[valSetList.V==0]))\n",
        "print(len(valSetList[valSetList.V==1]))\n",
        "print(len(valSetList[valSetList.V==2]))\n",
        "\n",
        "x_train_fileNames= trainSetList.imgPath.values #[:100]\n",
        "y_train= trainSetList.V.values #[:100]\n",
        "\n",
        "x_test_fileNames= valSetList.imgPath.values #[:100]\n",
        "y_test= valSetList.V.values #[:100]\n",
        "\n",
        "x_train= loadInputImages(x_train_fileNames, dirPath=dataPath)\n",
        "y_train= to_categorical(y_train, classeMax)\n",
        "\n",
        "print(y_train)\n",
        "\n",
        "x_test= loadInputImages(x_test_fileNames, dirPath=dataPath)\n",
        "y_test= to_categorical(y_test, classeMax)\n",
        "\n",
        "\n",
        "# il dataset subisce una fase di preprocessing per poter essere utilizzato sulla rete VGG16\n",
        "#x_train=preprocess_input(x_train)\n",
        "#x_test=preprocess_input(x_test)\n",
        "\n",
        "\n",
        "# COMPILE MODEL\n",
        "model.compile(loss=lossFunction, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "\n",
        "# DEFINE GENERATORS\n",
        "gen = keras.preprocessing.image.ImageDataGenerator(\n",
        "        rotation_range=90.,\n",
        "\t      horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "\t      featurewise_center=True,\n",
        "\t      featurewise_std_normalization=True\n",
        ")\n",
        "gen.fit(x_train)\n",
        "\n",
        "val_gen = keras.preprocessing.image.ImageDataGenerator(\n",
        "\t      featurewise_center=True,\n",
        "\t      featurewise_std_normalization=True\n",
        ")\n",
        "val_gen.fit(x_test)\n",
        "\n",
        "\n",
        "# CALLBACKS\n",
        "csv_logger = CSVLogger(csv)\n",
        "#saved_model = ModelCheckpoint(modello, save_best_only=True, save_weights_only=False, monitor=\"val_acc\")\n",
        "saved_model_best = ModelCheckpoint('results/finetuningBest.h5',save_best_only=True, save_weights_only=False, monitor=\"val_acc\")\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "steps= len(x_train)//batch_size\n",
        "val_steps= len(x_test)//val_batch_size\n",
        "\n",
        "#history=model.fit(x_train, y_train, batch_size=2, epochs=250, validation_data=(x_test, y_test), callbacks=[csv_logger, saved_model])\n",
        "history=model.fit_generator(gen.flow(x_train, y_train, batch_size=batch_size, shuffle=True),\n",
        "                    epochs=epochs, verbose=1,\n",
        "                    validation_data=val_gen.flow(x_test, y_test, batch_size=val_batch_size, shuffle=False),\n",
        "                    callbacks=[csv_logger, saved_model_best], #, saved_model\n",
        "                    steps_per_epoch= steps, validation_steps=val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4256\n",
            "1420\n",
            "1423\n",
            "1413\n",
            "1063\n",
            "353\n",
            "350\n",
            "360\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/250\n",
            " 87/133 [==================>...........] - ETA: 30s - loss: 1.0737 - acc: 0.4759"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J5jBTSfZpki",
        "colab_type": "text"
      },
      "source": [
        "Save weights to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvBHe-RqEYI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"results/epoc100.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr3wgkVwY9p-",
        "colab_type": "text"
      },
      "source": [
        "plot training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OGLVdWEY9EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Viene creato un grafico con l'andamento delle accuracy durante il finetuning:\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\t#train_accuracy\n",
        "plt.plot(history.history['val_acc']) #val_binary_accuracy\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "#plt.show()\n",
        "plt.savefig(plot1, format='png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3B8L8xoefip",
        "colab_type": "text"
      },
      "source": [
        "**evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flZF1nMIW_9Z",
        "colab_type": "code",
        "outputId": "4ba0f0e3-6953-4764-f61d-c196e146c94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#val_gen = keras.preprocessing.image.ImageDataGenerator(\n",
        "#\t      featurewise_center=True,\n",
        "#\t      featurewise_std_normalization=True\n",
        "#)\n",
        "#val_gen.fit(x_test)\n",
        "\n",
        "\n",
        "#model.load_weights(\"results/finetuning-e69-vba0.62.h5\")\n",
        "#res= model.evaluate_generator(val_gen.flow(x_test, y_test, batch_size=val_batch_size, shuffle=False), steps=val_steps, verbose=1)\n",
        "#print(res)\n",
        "\n",
        "model.load_weights(\"results/finetuningBest.h5\")\n",
        "res= model.evaluate_generator(val_gen.flow(x_test, y_test, batch_size=val_batch_size, shuffle=False), steps=val_steps, verbose=1)\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 10s 308ms/step\n",
            "[1.0170776067358074, 0.5975378787878788]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX6qDhGaxfHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Controllo i risultati\n",
        "!ls results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8A-vIJP9USc",
        "colab_type": "text"
      },
      "source": [
        "**prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NwvIN9T9T_J",
        "colab_type": "code",
        "outputId": "5146749b-e233-4d1f-f28c-2b6d495ab5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "y_pred= model.predict_generator(val_gen.flow(x_test, y_test, batch_size=val_batch_size, shuffle=False), steps=val_steps, verbose=1)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33/33 [==============================] - 10s 310ms/step\n",
            "[[0.05636145 0.24803051 0.695608  ]\n",
            " [0.02551727 0.62373644 0.35074627]\n",
            " [0.26109308 0.55215603 0.18675095]\n",
            " ...\n",
            " [0.03341064 0.94679487 0.01979459]\n",
            " [0.31647807 0.59580004 0.08772186]\n",
            " [0.13427217 0.24500684 0.620721  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peeb6b6K-bHa",
        "colab_type": "code",
        "outputId": "af3106b0-90bd-43c3-a6a3-6de00eabd73f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(type(y_pred))\n",
        "print(np.shape(y_pred))\n",
        "np.save('results/pred.npy',y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<type 'numpy.ndarray'>\n",
            "(1056, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqtgGOFg-lND",
        "colab_type": "text"
      },
      "source": [
        "**evaluate prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfjGeZzl-lzi",
        "colab_type": "code",
        "outputId": "7eaed1c6-df4c-4a3e-ebbc-c2a35800cd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "true= np.argmax(y_test, axis=1)[:len(y_pred)]\n",
        "pred= np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm= confusion_matrix(true, pred)\n",
        "cm= cm.astype('float32')\n",
        "print('\\nconfusion matrix:')\n",
        "print(cm)\n",
        "\n",
        "accuracy= accuracy_score(true, pred)\n",
        "print('\\naccuracy: '+str(accuracy))\n",
        "\n",
        "precisionMacro= precision_score(true, pred, average='macro')   \n",
        "precisionWeighted= precision_score(true, pred, average='weighted')\n",
        "precision= precision_score(true, pred, average=None)\n",
        "\n",
        "recallMacro= recall_score(true, pred, average='macro')   \n",
        "recallWeighted= recall_score(true, pred, average='weighted')\n",
        "recall= recall_score(true, pred, average=None)\n",
        "\n",
        "f1Macro= f1_score(true, pred, average='macro')  \n",
        "f1Weighted= f1_score(true, pred, average='weighted')\n",
        "f1= f1_score(true, pred, average=None)\n",
        "\n",
        "print('\\nclassification metric:')\n",
        "print('precision \\t recall \\t f1Score')\n",
        "for p,r,f in zip(precision,recall,f1):\n",
        "    print(str(p)+'\\t'+str(r)+'\\t'+str(f))\n",
        "\n",
        "print('\\nmacro metric:')\n",
        "print('precision \\t recall \\t f1Score')\n",
        "print(str(precisionMacro)+'\\t'+str(recallMacro)+'\\t'+str(f1Macro))\n",
        "\n",
        "print('\\nweighted metric:')\n",
        "print('precision \\t recall \\t f1Score')\n",
        "print(str(precisionWeighted)+'\\t'+str(recallWeighted)+'\\t'+str(f1Weighted))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "confusion matrix:\n",
            "[[224.  90.  27.]\n",
            " [ 52. 241.  61.]\n",
            " [ 69. 126. 166.]]\n",
            "\n",
            "accuracy: 0.5975378787878788\n",
            "\n",
            "classification metric:\n",
            "precision \t recall \t f1Score\n",
            "0.6492753623188405\t0.656891495601173\t0.653061224489796\n",
            "0.5273522975929978\t0.6807909604519774\t0.594327990135635\n",
            "0.6535433070866141\t0.4598337950138504\t0.5398373983739837\n",
            "\n",
            "macro metric:\n",
            "precision \t recall \t f1Score\n",
            "0.6100569889994841\t0.5991720836890003\t0.5957422043331382\n",
            "\n",
            "weighted metric:\n",
            "precision \t recall \t f1Score\n",
            "0.609862448633441\t0.5975378787878788\t0.5946659913561017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKoaW5Pv5rNh",
        "colab_type": "text"
      },
      "source": [
        "**feature extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIJaDQdJ5qxY",
        "colab_type": "code",
        "outputId": "e532ecff-139a-4a5a-e908-f125b6db6dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "###\n",
        "dataPath=''\n",
        "datasetFile=\"dataset.txt\"\n",
        "datasetList= loadDataset(datasetFile)\n",
        "\n",
        "xfn= datasetList.imgPath.values #[:100]\n",
        "y= datasetList.O.values #[:100]\n",
        "\n",
        "x= loadInputImages(xfn, dirPath=dataPath)\n",
        "y= to_categorical(y, classeMax)\n",
        "###\n",
        "\n",
        "#model.compile(loss=lossFunction, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "model.load_weights(\"results/finetuningBest.h5\")\n",
        "\n",
        "model2 = Model(inputs=base_model.input, outputs=model.get_layer(\"fc2\").output)\n",
        "\n",
        "X= model2.predict(x,batch_size=32)#, y, batch_size=val_batch_size, shuffle=False), steps=val_steps, verbose=1)\n",
        "print(feat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-71ac3d49b47d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, y, batch_size=val_batch_size, shuffle=False), steps=val_steps, verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'feat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r261V5nL9dHn",
        "colab_type": "code",
        "outputId": "60551c7e-f14a-4114-91c1-be13d5265e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(type(X))\n",
        "print(np.shape(X))\n",
        "np.save('results/Xvis.npy',X)\n",
        "\n",
        "print(type(y))\n",
        "print(np.shape(y))\n",
        "np.save('results/y.npy',y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<type 'numpy.ndarray'>\n",
            "(9247, 4096)\n",
            "<type 'numpy.ndarray'>\n",
            "(9247, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yi92LsaeXN3",
        "colab_type": "code",
        "outputId": "674eac70-b578-433f-ddbd-c772502f0b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y= datasetList.O.values\n",
        "print(type(y))\n",
        "print(np.shape(y))\n",
        "np.save('results/y.npy',y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<type 'numpy.ndarray'>\n",
            "(9247,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuewHnT4e4i2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt('results/path.txt', xfn, fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly9gCqKCeijP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pynv98rF2F6q"
      },
      "source": [
        "## **Download results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY71N5l9pM4G",
        "colab_type": "text"
      },
      "source": [
        "download diretto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH5nQXsBpMA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Come scaricare un file da Google Colab\n",
        "#Esempio: scarico il grafico delle accuracy\n",
        "from google.colab import files\n",
        "path=\"results/finetuning-finetuning.csv\"\n",
        "files.download(path)\n",
        "#NB: non sempre funziona correttamente lo scaricamento da Google Colab. Se il file è troppo grande, potrebbe capitare che il download fallisca."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfh6g5xT1ZDA",
        "colab_type": "text"
      },
      "source": [
        "**Salvataggio sessione in GDrive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc8Ssn7eE2h2",
        "colab_type": "text"
      },
      "source": [
        "compressione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D4PHK5dE2Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip visual.zip results/finetuning-e69-vba0.62.h5 results/finetuningBest.h5 results/finetuning-finetuning.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMhCmRpE3dw",
        "colab_type": "text"
      },
      "source": [
        "trasferimento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EJwwEm71mGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ALTERNATIVA PER SCARICARE UN FILE: facciamo l'upload da Google Colab a Google Drive (però dovete aver fatto in precedenza l'autenticazione con PyDrive)\n",
        "#Esempio: salvo il model su drive\n",
        "\n",
        "#!rm session2.zip\n",
        "#!zip session_unet0.zip data/** models/** results/**\n",
        "\n",
        "\n",
        "file1 = drive.CreateFile()\n",
        "file1.SetContentFile('visual.zip')\n",
        "file1.Upload()\n",
        "\n",
        "#Per esperienza, anche questo metodo a volte potrebbe non funzionare...\n",
        "#In questo caso provare a rifare l'autenticazione"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSJ3svlf3UOC",
        "colab_type": "text"
      },
      "source": [
        "#**Console**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK8PvQjo6UhC",
        "colab_type": "text"
      },
      "source": [
        "upload from pc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HNxEvorfpUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#UPLOAD DATASET FROM PC\n",
        "#da ripetere sia per il file train.zip sia per il test.zip\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OkMBcFQ3Xzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxA1VSL03yEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv models/unet1500.hdf5 models/unet50-4d.hdf5\n",
        "!mv models/unetW.hdf5 models/unetBest-4d.hdf5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WamSyCUW-EQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f96SfmX6-IRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm models/unetW.hdf5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkSM9H938TeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm results/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm5gtq9V4I1S",
        "colab_type": "code",
        "outputId": "2e8eb4ec-44b7-493d-e44a-5eb1094cfbdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!rm -r *\n",
        "!rm *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4B6kXgI3Adx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
